# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1V1A5c9MotOzFYwwaTg9MyK8Ob6ASe2fH
"""

# Description: This program classify MNIST handwritten digits images
#as a number 0-9
!pip install tensorflow keras numpy mnist matplotlib

# Import the packages/ dependencies
import numpy as np
import mnist # Get data set from
import matplotlib.pyplot as plt #graph
from keras.models import Sequential # ANN architecture
from keras.layers import Dense #The layers in the ANN
from keras.utils import to_categorical

#load the data set
train_images = mnist.train_images() #training data images
train_lables = mnist.train_labels() #training data lables
test_images = mnist.test_images() #training data images
test_lables = mnist.test_labels() #trainning data lables

#Normalize the images. Normalize the pixel values from [0,255] tp
# [-0.5,0.5] tom make our network easier to train
train__images = (train_images/255) - 0.5
test_images = (test_images/255) - 0.5
#flatten the image. Flatten each 28x28 image into a 28^2=784 dimensional vector
#To pass into neural networks
train_images = train_images.reshape((-1,784))
test_images = test_images.reshape((-1,784))
#print the shape
print(train_images.shape)# 60000 rows and 784 column
print(test_images.shape) # 10000 rows and 784 column

#build the model
# 3 layers, 2 layers with 64 neurons and the relu function
# 1 layer with 10 neurons and softmax function
model = Sequential()
model.add( Dense(64, activation='relu', input_dim=784))
model.add( Dense(64, activation='relu'))
model.add(Dense(10, activation='softmax'))

#compile the model
#The lost funcyion measures how well the model did in training 
#and then tries to improve on it using optimizer 
model.compile(
    optimizer = 'adam',
    loss  = 'categorical_crossentropy', #(classes that are greater than 2)
    metrics = ['accuracy']
)

#train the model
model.fit(
    train_images,
    to_categorical(train_lables), #Ex . 2 it esxpect [0,0,1,0,0,0,0,0,0]
    epochs = 5, #Number of itteration over the entire dataset to train on
    batch_size=32 #number of sample per gradient upadte for training
)

#evaluate the model
model.evaluate(
    test_images,
    to_categorical(test_lables)
)

#model.save_weight('model.hs')

#prdict on the firs 5 test images
predictions = model.predict(test_images[:5])
#print our model predictions
print(np.argmax(predictions, axis=1))
print(test_lables[:5])

for i in range(0,5):
  first_image = test_images[i]
  first_image = np.array(first_image,dtype='float')
  pixels = first_image.reshape((28,28))
  plt.imshow(pixels, cmap='gray')
  plt.show()